{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4490536",
   "metadata": {},
   "source": [
    "# Python `PySpark` Module: Methods, Functions, and Examples\n",
    "\n",
    "This notebook provides detailed notes, explanations, and examples for working with Apache Spark using PySpark. Each cell covers a specific concept or function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323bc3d",
   "metadata": {},
   "source": [
    "## Introduction to `PySpark`\n",
    "\n",
    "PySpark is the Python API for Apache Spark, a fast and general-purpose cluster computing system. It provides high-level APIs for distributed computing and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de911523",
   "metadata": {},
   "source": [
    "## Importing PySpark\n",
    "\n",
    "First, we need to import the necessary PySpark modules and create a SparkSession, which is the entry point for PySpark functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebc2c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Examples\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dcc51e",
   "metadata": {},
   "source": [
    "## Creating DataFrames\n",
    "\n",
    "PySpark DataFrames are distributed collections of data organized into named columns. Let's create a DataFrame from a simple list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9af35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from a list of data\n",
    "data = [(\"John\", 30, \"New York\"),\n",
    "        (\"Anna\", 25, \"San Francisco\"),\n",
    "        (\"Peter\", 35, \"Chicago\")]\n",
    "columns = [\"name\", \"age\", \"city\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "print(\"DataFrame Schema:\")\n",
    "df.printSchema()\n",
    "print(\"\\nDataFrame Content:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e033351",
   "metadata": {},
   "source": [
    "## Basic DataFrame Operations\n",
    "\n",
    "Let's explore common DataFrame operations like selecting columns, filtering rows, and adding new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, lit\n",
    "\n",
    "# Select specific columns\n",
    "print(\"Select specific columns:\")\n",
    "df.select(\"name\", \"age\").show()\n",
    "\n",
    "# Filter rows\n",
    "print(\"\\nFilter people older than 30:\")\n",
    "df.filter(col(\"age\") > 30).show()\n",
    "\n",
    "# Add new column\n",
    "print(\"\\nAdd greeting column:\")\n",
    "df_with_greeting = df.withColumn(\"greeting\", \n",
    "    concat(lit(\"Hello, \"), col(\"name\")))\n",
    "df_with_greeting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5de4dc",
   "metadata": {},
   "source": [
    "## Aggregations and Grouping\n",
    "\n",
    "PySpark provides powerful functions for aggregating and grouping data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "# Create a new DataFrame with more data\n",
    "data_agg = [(\"NY\", \"Sales\", 1000),\n",
    "            (\"NY\", \"Sales\", 2000),\n",
    "            (\"SF\", \"Sales\", 3000),\n",
    "            (\"SF\", \"HR\", 1500),\n",
    "            (\"CH\", \"HR\", 2500)]\n",
    "df_agg = spark.createDataFrame(data_agg, [\"city\", \"dept\", \"salary\"])\n",
    "\n",
    "# Group by and aggregate\n",
    "print(\"Average salary by city:\")\n",
    "df_agg.groupBy(\"city\").agg(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    count(\"*\").alias(\"count\")\n",
    ").show()\n",
    "\n",
    "# Group by multiple columns\n",
    "print(\"\\nCount by city and department:\")\n",
    "df_agg.groupBy(\"city\", \"dept\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58080337",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "\n",
    "Window functions perform calculations across a set of rows related to the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db79669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank\n",
    "\n",
    "# Create window specification\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "# Add rank and dense_rank\n",
    "df_with_rank = df_agg.withColumn(\"rank\", rank().over(windowSpec)) \\\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(windowSpec))\n",
    "\n",
    "print(\"Rankings within departments:\")\n",
    "df_with_rank.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4d4c23",
   "metadata": {},
   "source": [
    "## Joining DataFrames\n",
    "\n",
    "PySpark supports different types of joins between DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c60314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two DataFrames for joining\n",
    "employee_data = [(\"001\", \"John\"), (\"002\", \"Anna\"), (\"003\", \"Bob\")]\n",
    "department_data = [(\"001\", \"Sales\"), (\"002\", \"HR\"), (\"004\", \"IT\")]\n",
    "\n",
    "df_employees = spark.createDataFrame(employee_data, [\"id\", \"name\"])\n",
    "df_departments = spark.createDataFrame(department_data, [\"id\", \"department\"])\n",
    "\n",
    "# Inner join\n",
    "print(\"Inner Join:\")\n",
    "df_employees.join(df_departments, \"id\", \"inner\").show()\n",
    "\n",
    "# Left outer join\n",
    "print(\"\\nLeft Outer Join:\")\n",
    "df_employees.join(df_departments, \"id\", \"left\").show()\n",
    "\n",
    "# Right outer join\n",
    "print(\"\\nRight Outer Join:\")\n",
    "df_employees.join(df_departments, \"id\", \"right\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345897f",
   "metadata": {},
   "source": [
    "## User-Defined Functions (UDFs)\n",
    "\n",
    "Create custom functions to transform data in PySpark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9308718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a Python function\n",
    "def make_greeting(name):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "# Register as UDF\n",
    "greeting_udf = udf(make_greeting, StringType())\n",
    "\n",
    "# Apply UDF to DataFrame\n",
    "df_with_udf = df_employees.withColumn(\"greeting\", greeting_udf(col(\"name\")))\n",
    "print(\"DataFrame with UDF applied:\")\n",
    "df_with_udf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec9a38",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "PySpark offers powerful distributed computing capabilities:\n",
    "- DataFrame operations for structured data\n",
    "- SQL-like operations and aggregations\n",
    "- Window functions for advanced analytics\n",
    "- Joining multiple datasets\n",
    "- Custom transformations with UDFs\n",
    "\n",
    "Clean up the SparkSession when done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc2aa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
